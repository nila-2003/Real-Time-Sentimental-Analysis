# -*- coding: utf-8 -*-
"""real_time_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ApVtqHJv9zfg9N3M002Lkj1mC9yNoemP
"""

import numpy as np
import pandas as pd
import librosa
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from tensorflow.keras.models import model_from_json
from tensorflow.keras.models import Sequential


def real_time_detection(audio_file_path, df_for_training):
    try:
        with open('/content/drive/MyDrive/llm/model_architecture.json', 'r') as json_file:
            loaded_model_json = json_file.read()

        loaded_model = Sequential()
        loaded_model = model_from_json(loaded_model_json)
        loaded_model.load_weights('/content/drive/MyDrive/llm/pretrained_Sentiment_model.h5')

        scaler = StandardScaler().fit(df_for_training.iloc[:, :-1])
        pca = PCA(n_components=22).fit(scaler.transform(df_for_training.iloc[:, :-1]))
        rf_classifier = RandomForestClassifier()
        rfe = RFE(rf_classifier, n_features_to_select=17).fit(pca.transform(scaler.transform(df_for_training.iloc[:, :-1])), df_for_training['emotion'])

        num_features_for_training = len(df_for_training.columns) - 1
        n_components_for_pca = 22
        def feature_extraction_from_file(audio_file_path, expected_length=22):
          audio_data, sampling_rate = librosa.load(audio_file_path, sr=None)
          spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio_data, sr=sampling_rate))
          spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio_data, sr=sampling_rate))
          spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio_data, sr=sampling_rate))

          mfcc = librosa.feature.mfcc(y=audio_data, sr=sampling_rate)

          mfcc_features = [np.mean(el) for el in mfcc]
          features = [spectral_centroid, spectral_bandwidth, spectral_rolloff] + mfcc_features
          if len(features) > expected_length:
            return features[:expected_length]
          elif len(features) < expected_length:
            return features + [0] * (expected_length - len(features))
          else:
            return features

        def preprocess_and_augment(features, shift_range=0.1, noise_range=0.01):
            X = df_for_training.iloc[:, :-1]
            y = df_for_training['emotion']

            imputer = SimpleImputer(strategy='mean')
            X_imputed = imputer.fit_transform(X)

            X_scaled = scaler.transform(X_imputed)

            X_pca = pca.fit_transform(X_scaled)
            X_rfe = rfe.fit_transform(X_scaled, y)

            augmented_features = []
            for x in X_rfe:
                augmented_features.append(x)

                for _ in range(3):
                    shift = np.random.uniform(-shift_range, shift_range)
                    x_shifted = x + shift
                    augmented_features.append(x_shifted)

                x_noise = x + np.random.uniform(-noise_range, noise_range, len(x))
                augmented_features.append(x_noise)

            X_augmented = np.array(augmented_features)
            feature_names = X.columns[rfe.support_]

            df_augmented = pd.DataFrame(X_augmented, columns=feature_names)
            df_augmented['emotion'] = y.values

            return df_augmented

        emotion_column_index = df_for_training.columns.get_loc('emotion')

        features = feature_extraction_from_file(audio_file_path)

        if len(features) != num_features_for_training:
            raise ValueError(f"Expected {num_features_for_training} features, but got {len(features)} features.")

        df_real_time = pd.DataFrame([features])
        df_real_time.iloc[:, :num_features_for_training] = scaler.transform(df_real_time.iloc[:, :num_features_for_training])

        pca_transformed = pca.transform(df_real_time.iloc[:, :num_features_for_training])

        if pca_transformed.shape[1] != num_features_for_training:
            raise ValueError(f"Expected {num_features_for_training} features after PCA, but got {pca_transformed.shape[1]} features.")

        rfe_transformed = rfe.transform(pca_transformed)
        model_input = rfe_transformed.reshape(1, rfe_transformed.shape[1], 1)

        predictions = loaded_model.predict(model_input)

        predicted_label = np.argmax(predictions)

        return predicted_label

    except KeyboardInterrupt:
        print("Stopped by the user.")
        return None

df_for_training = pd.read_csv("/content/drive/MyDrive/llm/preprocessed_features.csv")
audio_file_path = "/content/drive/MyDrive/llm/1001_IEO_HAP_LO.wav"
predicted_emotion = real_time_detection(audio_file_path, df_for_training)

print("Predicted Emotion:", predicted_emotion)

